{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing datasets\n",
    "\n",
    "In this notebook, we're going to download the [Amazon US reviews](https://huggingface.co/datasets/amazon_us_reviews) dataset, explore it a bit, process it, and push it back to the Hugging Face Hub.\n",
    "\n",
    "```datasets```documentation: https://huggingface.co/docs/datasets/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to a push a dataset on the Hugging Face Hub, we need to install Git Large File Support (LFS):\n",
    "\n",
    "1) Git LFS setup:\n",
    "\n",
    "In a terminal:\n",
    "\n",
    "```\n",
    "curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bash\n",
    "sudo yum install git-lfs -y\n",
    "git lfs install\n",
    "```\n",
    "Then, add ```*.csv filter=lfs diff=lfs merge=lfs -text``` to ```.gitattributes```, so that CSV files will also be managed with Git LFS.\n",
    "\n",
    "2) Install the [Hugging Face CLI](https://github.com/huggingface/huggingface_hub)\n",
    "\n",
    "```pip -q install huggingface_hub```\n",
    "\n",
    "3) Login to the Hub with your hub credentials\n",
    "\n",
    "```huggingface-cli login```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "pip -q install datasets huggingface_hub --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Loading and exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('amazon_us_reviews', 'Shoes_v1_00', split='train')\n",
    "print('{:.2f} GB'.format(dataset.size_in_bytes/1024/1024/1024))\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is pretty large. We definitely don't need (or want) that much to begin with. Let's work with 10% only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('amazon_us_reviews', 'Shoes_v1_00', split='train[:10%]')\n",
    "print(dataset.shape)\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first entry in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot of columns that we don't need right now. Let's just keep ```review_body``` and ```star_rating```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['marketplace', 'customer_id', 'review_id', 'product_id', 'product_parent', 'product_title', 'product_category', 'helpful_votes', 'total_votes', 'vine', 'verified_purchase', 'review_headline', 'review_date'])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we don't have any unexpect ```star rating``` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.unique('star_rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see what the distribution of star ratings is. This also shows how we can easily convert a datasets to pandas, in order to use well-known functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_pandas().value_counts('star_rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is quite unbalanced. We have too many 4 and 5-star ratings. Let's rebalance the dataset, and keep only 20,000 reviews in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dataset_pd = dataset.to_pandas()\n",
    "dataset_pd_balanced = pd.DataFrame(columns=dataset.column_names)\n",
    "for stars in range(1,6):\n",
    "    data = dataset_pd[dataset_pd['star_rating']==stars][:20000]\n",
    "    dataset_pd_balanced = pd.concat([dataset_pd_balanced, data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pd_balanced.value_counts('star_rating')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can switch back to the Hugging Face dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_balanced = datasets.Dataset.from_pandas(dataset_pd_balanced, preserve_index=False)\n",
    "print(dataset_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer models require that labels start at 0, so let's decrement all star ratings using the ```map()``` function in datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels must start at 0\n",
    "\n",
    "def decrement_stars(row):\n",
    "    return {\n",
    "        'star_rating': row['star_rating']-1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_balanced = dataset_balanced.map(decrement_stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's rename columns to what the model expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_balanced = dataset_balanced.rename_column('star_rating', 'labels')\n",
    "dataset_balanced = dataset_balanced.rename_column('review_body', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_balanced[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we split the dataset for training and validation. Let's set 10% aside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = dataset_balanced.train_test_split(test_size=0.1, shuffle=True, seed=59)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Saving the dataset and pushing it to the to hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we save the dataset to disk in Hugging Face (Apache Arrow) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split.save_to_disk('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split.push_to_hub(repo_id='amazon-shoe-reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is now visible at https://huggingface.co/datasets/juliensimon/amazon-shoe-reviews.\n",
    "\n",
    "Of course, we could also use a Git workflow:\n",
    "\n",
    "```\n",
    "huggingface-cli repo create -y amazon-shoe-reviews --type dataset\n",
    "\n",
    "git clone https://huggingface.co/datasets/juliensimon/amazon-shoe-reviews\n",
    "    \n",
    "cd amazon-shoe-reviews\n",
    "\n",
    "cp -r ../data/* .\n",
    "\n",
    "git add .\n",
    "\n",
    "git commit -m 'Initial version'\n",
    "\n",
    "git push\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good practice to describe datasets in a dataset card: language(s), task types, etc. You can easily create one by clicking on \"Add a dataset card\" on the dataset page. \n",
    "\n",
    "Alternatively, you can add a README.md file to the dataset repository. This file should follow a well-defined format described at https://huggingface.co/docs/datasets/upload_dataset#create-a-dataset-card."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's also save the dataset in CSV format. That may come in handy later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs('data_csv', exist_ok=True)\n",
    "dataset_split['train'].to_csv('data_csv/amazon_shoe_reviews_train.csv', header=True, index=False)\n",
    "dataset_split['test'].to_csv('data_csv/amazon_shoe_reviews_test.csv', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next notebook, we're going to use the dataset to train a first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
